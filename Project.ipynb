{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <h1 style=\"background-color: rgba(128, 128, 128, 0.29)\"><center>**_Sentiment Analysis Project_ üôÇüòêüò†**</center></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## **IMPORT THE LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, f1_score , classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **DATA ANALYSIS**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color='#ddd'>**‚ñ∂Ô∏è Data Ingesion**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sentimentdataset.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Data Description**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Punctuation**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Stop Words List**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Slang Dictionary**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"icymi\": \"in case you missed it\",\n",
    "    \"jsyk\": \"just so you know\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"pls\": \"please\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"afaik\": \"as far as I know\",\n",
    "    \"b4\": \"before\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"dunno\": \"don't know\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"nite\": \"night\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"tho\": \"though\",\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"wut\": \"what\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"fwiw\": \"for what it's worth\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"rly\": \"really\",\n",
    "    \"tbf\": \"to be fair\",\n",
    "    \"tldr\": \"too long; didn't read\",\n",
    "    \"wyd\": \"what are you doing\",\n",
    "    \"wym\": \"what do you mean\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"btwn\": \"between\",\n",
    "    \"cya\": \"see ya\",\n",
    "    \"fav\": \"favorite\",\n",
    "    \"gratz\": \"congratulations\",\n",
    "    \"plz\": \"please\",\n",
    "    \"thru\": \"through\",\n",
    "    \"totes\": \"totally\",\n",
    "    \"ttyt\": \"talk to you tomorrow\",\n",
    "    \"wb\": \"welcome back\",\n",
    "    \"wbu\": \"what about you\",\n",
    "    \"yw\": \"you're welcome\",\n",
    "    \"bffl\": \"best friends for life\",\n",
    "    \"m8\": \"mate\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"rsvp\": \"please respond\",\n",
    "    \"tfw\": \"that feeling when\",\n",
    "    \"idc\": \"I don't care\",\n",
    "    \"wtf\": \"what the heck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Lemmatizer Object**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Preprocessing Function**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(text):\n",
    "    # Remove The Punctuation\n",
    "    clean_txt = \"\".join([char for char in text if char not in punct])\n",
    "    # Remove The Emojis\n",
    "    clean_txt  = re.sub(\n",
    "        r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-'\n",
    "        r'\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-'\n",
    "        r'\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+',\n",
    "        '', clean_txt)\n",
    "    # Convert Text To Lower Case\n",
    "    clean_txt = clean_txt.lower()\n",
    "    # Split The Text To Words\n",
    "    clean_txt = clean_txt.split()\n",
    "    # Convert the Slang Terms\n",
    "    clean_txt = [slang_dict.get(word, word) for word in clean_txt]\n",
    "    # Remove The Stop Words\n",
    "    for word in stop_words:\n",
    "        if word.__contains__(\"n't\"):\n",
    "            stop_words.remove(word)\n",
    "        if word == \"not\":\n",
    "            stop_words.remove(word)\n",
    "    clean_txt = [word for word in clean_txt if word not in stop_words]\n",
    "    # Lemmatizing\n",
    "    clean_txt = [lemmatizer.lemmatize(word) for word in clean_txt]\n",
    "    return \" \".join(clean_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Apply the Preprocessing Function On Text Column**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Clean Text'] = data['Text'].apply(lambda x: data_preprocessing(x))\n",
    "data[['Text' , 'Clean Text']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Apply the Preprocessing Function On Topic Column**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Clean Topic'] = data['Topic'].apply(lambda x: data_preprocessing(x))\n",
    "data[['Topic' , 'Clean Topic']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Delete Useless Columns**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['User', 'Timestamp', 'Text', 'Topic', 'Year', 'Month', 'Day', 'Hour', 'Source', 'ID'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **FEATURE ENGINEERING & ENCODING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Convert The Column Of The Sentiment To Three Unique Values**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = SentimentIntensityAnalyzer()\n",
    "def convert_sentiment(word):\n",
    "    sentiment_scores = analysis.polarity_scores(lemmatizer.lemmatize(word))\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Apply the Convert function on Sentiment Column**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['New Sentiments'] = data['Sentiment (Label)'].apply(lambda x: convert_sentiment(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Sentiments Description**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sentiment (Label)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['New Sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Convert The Sentiment Values To Numerical Values**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentiment_to_number(sentiment):\n",
    "    if sentiment == 'Positive':\n",
    "        return 1\n",
    "    elif sentiment == 'Negative':\n",
    "        return -1\n",
    "    elif sentiment == 'Neutral':\n",
    "        return 0\n",
    "\n",
    "data['Numerical_Sentiments'] = data['New Sentiments'].apply(lambda x: convert_sentiment_to_number(x))\n",
    "data['Numerical_Sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Delete 'Sentiment (Label)' & 'New Sentiments'**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Sentiment (Label)', 'New Sentiments'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Correlation**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data[['Likes' , 'Retweets' , 'Numerical_Sentiments']].corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Delete 'Likes' & 'Retweets' Columns**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Likes', 'Retweets'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Data Spliting**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['Clean Text'] + \" \" + data['Clean Topic'] , \n",
    "data['Numerical_Sentiments'], test_size=0.1, random_state=0,\n",
    "stratify= data['Numerical_Sentiments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Bag Of Words**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,1))\n",
    "x_train_bagOfWords11 = cv.fit_transform(x_train).toarray()\n",
    "x_test_bagOfWords11 = cv.transform(x_test).toarray()\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "x_train_bagOfWords12 = cv.fit_transform(x_train).toarray()\n",
    "x_test_bagOfWords12 = cv.transform(x_test).toarray()\n",
    "\n",
    "cv13 = CountVectorizer(ngram_range=(1,3))\n",
    "x_train_bagOfWords13 = cv13.fit_transform(x_train).toarray()\n",
    "x_test_bagOfWords13 = cv13.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è TF-IDF**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "x_train_TFIDF11 = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_TFIDF11 = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "x_train_TFIDF12 = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_TFIDF12 = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "x_train_TFIDF13 = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_TFIDF13 = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **TRAINING & MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>==================================================</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Naive Bayes**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>Bag Of Words</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bagOfWords =  MultinomialNB()\n",
    "naive_bagOfWords.fit(x_train_bagOfWords13, y_train)\n",
    "y_pred = naive_bagOfWords.predict(x_test_bagOfWords13)\n",
    "print(classification_report(y_pred, y_test)) \n",
    "classes = [-1, 0, 1]\n",
    "sns.heatmap(confusion_matrix(y_pred, y_test), annot=True, xticklabels=classes, yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>TF-IDF</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_TFIDF11,y_train)\n",
    "y_pred = clf.predict(x_test_TFIDF11)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>==================================================</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Logistic Regression**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>Bag Of Words</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_bagOfWords = LogisticRegression()\n",
    "logistic_bagOfWords.fit(x_train_bagOfWords12,y_train)\n",
    "y_pred = logistic_bagOfWords.predict(x_test_bagOfWords12)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>TF-IDF</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_TFIDF11,y_train)\n",
    "y_pred = clf.predict(x_test_TFIDF11)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>==================================================</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è SVC**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>Bag Of Words</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_train_bagOfWords11,y_train)\n",
    "y_pred = clf.predict(x_test_bagOfWords11)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>TF-IDF</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_train_TFIDF11,y_train)\n",
    "y_pred = clf.predict(x_test_TFIDF11)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>==================================================</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Random Forest Classifier**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>Bag Of Words</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "clf.fit(x_train_bagOfWords11,y_train)\n",
    "y_pred = clf.predict(x_test_bagOfWords11)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>TF-IDF</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "clf.fit(x_train_TFIDF11, y_train)\n",
    "y_pred = clf.predict(x_test_TFIDF11)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>==================================================</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è  Decision Tree Classifier**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>Bag Of Words</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(x_train_bagOfWords12,y_train)\n",
    "y_pred = clf.predict(x_test_bagOfWords12)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>TF-IDF</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(x_train_TFIDF12,y_train)\n",
    "y_pred = clf.predict(x_test_TFIDF12)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>==================================================</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#ddd'>**‚ñ∂Ô∏è Gradient Boosting Classifier**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>Bag Of Words</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.7)\n",
    "clf.fit(x_train_bagOfWords13, y_train)\n",
    "y_pred = clf.predict(x_test_bagOfWords13)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <font color='#ddd'><li>TF-IDF</li></font>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.7)\n",
    "clf.fit(x_train_TFIDF11, y_train)\n",
    "y_pred = clf.predict(x_test_TFIDF11)\n",
    "print(classification_report(y_pred,y_test))\n",
    "classes=[-1,0,1]\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test), annot=True,xticklabels=classes,yticklabels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Understand Public Opinion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_trends(keyword):\n",
    "    # Initialize counts for positive, negative, and neutral sentiments\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    neutral = 0\n",
    "    # Assuming 'Topic' column contains strings, filter rows containing the keyword\n",
    "    trends = data[data['Clean Topic'].str.contains((keyword).lower(), case=False)]\n",
    "    # Iterate over each sentiment in the filtered DataFrame\n",
    "    for sentiment in trends['Numerical_Sentiments']:\n",
    "        # Convert sentiment to polarity\n",
    "        # Update sentiment counts\n",
    "        if sentiment == 1:\n",
    "            positive += 1\n",
    "        elif sentiment == -1:\n",
    "            negative += 1\n",
    "        else:\n",
    "            neutral += 1\n",
    "    # Check which sentiment has the highest count and print the result\n",
    "    if positive > negative and positive > neutral:\n",
    "        print(\"Positive\")\n",
    "    elif negative > positive and negative > neutral:\n",
    "        print(\"Negative\")\n",
    "    elif neutral > positive and neutral > negative or positive == negative != 0:\n",
    "        print(\"Neutral\")\n",
    "    else:\n",
    "        print(\"not found\")\n",
    "\n",
    "keyword = input(\"Enter your keyword: \")\n",
    "sentiment_trends(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Improve Customer Experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the 'data' DataFrame, 'convert_sentiment' function, and other necessary components defined\n",
    "def improve(product):\n",
    "    trends = data[data['Clean Topic'].str.contains((product).lower(), case=False)]\n",
    "    for index, sentiment in trends['Numerical_Sentiments'].items():\n",
    "        if sentiment == -1:\n",
    "            print(trends.loc[index, 'Country'])\n",
    "product = input(\"Enter your product: \")\n",
    "improve(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_file = 'SentimentFile.sav'\n",
    "cv_file = 'Bag of Words.sav'\n",
    "pickle.dump(naive_bagOfWords,open(model_file,'wb'))\n",
    "pickle.dump(cv13, open(cv_file, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
